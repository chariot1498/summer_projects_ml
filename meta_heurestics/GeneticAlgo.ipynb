{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define some initial params\n",
    "# Dimensionality (No. of features)\n",
    "Dn = 20\n",
    "\n",
    "# Mutation probability (This is done to add more randomness usually kept small)\n",
    "M = 0.1\n",
    "\n",
    "# Population size (No. of points you hope that atleast one of them will get to global minima)\n",
    "NP = 200\n",
    "\n",
    "# No. of generations (Iterations)\n",
    "G = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56760162  0.89980982  0.29421186  0.09884325  0.51126524  0.26882032\n",
      "  0.50041847  0.08404795  0.10173197  0.77664398  0.50422825  0.18651055\n",
      "  0.24022388  0.60483828  0.90823546  0.6325607   0.28949008  0.33650264\n",
      "  0.59346302  0.81483158]\n"
     ]
    }
   ],
   "source": [
    "#We will try to optimize function sum over all dimensions ((x+r)^2) it will be our f(x) Where r is a random dn length\n",
    "#vector\n",
    "r = np.random.random((Dn,))\n",
    "def f(x):\n",
    "    return ((x + r)**2).sum()\n",
    "\n",
    "print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# We are going to randomly Generate population that is NP points each with dn features from a uniform sample from -10 \n",
    "# to 10.\n",
    "pop = []\n",
    "\n",
    "for ix in range(NP):\n",
    "    # generate a random vector\n",
    "    vec = np.random.uniform(-10, 10, (Dn,))\n",
    "    \n",
    "    # Add to population\n",
    "    pop.append(vec)\n",
    "#pop now has our initial random population\n",
    "print len(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These our two features crossover and mutate through these we introduce randomness in the child vectors. In crossover\n",
    "#We take half the features of one parent and half of the other and make two childs from it .\n",
    "#in mutate we try to introduce more randomness and is based on mutation in humans . We generate a random number btw\n",
    "#0 and 1 using np.random.random (R) if our mutation prob is more than this random number we replace that feature with\n",
    "#a random number generated from the uniform distribution between -5 and 5 .\n",
    "\n",
    "def crossover(parent_1, parent_2):\n",
    "    child_1 = np.zeros(parent_1.shape)\n",
    "    child_2 = np.zeros(parent_2.shape)\n",
    "    \n",
    "    dim = int(parent_1.shape[0] / 2.0)\n",
    "    \n",
    "    child_1[:dim] = parent_1[:dim]\n",
    "    child_1[dim:] = parent_2[dim:]\n",
    "    \n",
    "    child_2[:dim] = parent_2[:dim]\n",
    "    child_2[dim:] = parent_1[dim:]\n",
    "    \n",
    "    return child_1, child_2\n",
    "\n",
    "def mutate(x):\n",
    "    for ix in range(x.shape[0]):\n",
    "        # Generate a random number for probability\n",
    "        R = np.random.random()\n",
    "        \n",
    "        if R < M:\n",
    "            # Mutate random index \n",
    "            x[ix] = np.random.uniform(-5, 5)\n",
    "        else:\n",
    "            pass\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation: 0 | Best Value: 300.325257779\n",
      "Generation: 1 | Best Value: 261.228994791\n",
      "Generation: 2 | Best Value: 109.657314951\n",
      "Generation: 3 | Best Value: 90.5998324006\n",
      "Generation: 4 | Best Value: 74.5887254812\n",
      "Generation: 5 | Best Value: 54.6211697529\n",
      "Generation: 6 | Best Value: 37.4796894592\n",
      "Generation: 7 | Best Value: 33.0959047902\n",
      "Generation: 8 | Best Value: 30.5932017614\n",
      "Generation: 9 | Best Value: 23.5460150254\n",
      "Generation: 10 | Best Value: 18.4404559618\n",
      "Generation: 11 | Best Value: 18.4404559618\n",
      "Generation: 12 | Best Value: 15.3292833043\n",
      "Generation: 13 | Best Value: 12.8018870973\n",
      "Generation: 14 | Best Value: 10.8625587881\n",
      "Generation: 15 | Best Value: 10.4391553436\n",
      "Generation: 16 | Best Value: 10.4391553436\n",
      "Generation: 17 | Best Value: 9.04124962267\n",
      "Generation: 18 | Best Value: 8.6137546792\n",
      "Generation: 19 | Best Value: 7.87144847297\n",
      "Generation: 20 | Best Value: 7.57631493519\n",
      "Generation: 21 | Best Value: 7.57631493519\n",
      "Generation: 22 | Best Value: 7.14308527121\n",
      "Generation: 23 | Best Value: 6.89481697903\n",
      "Generation: 24 | Best Value: 6.34837685213\n",
      "Generation: 25 | Best Value: 4.83544747042\n",
      "Generation: 26 | Best Value: 4.83544747042\n",
      "Generation: 27 | Best Value: 4.5748584315\n",
      "Generation: 28 | Best Value: 3.73743573291\n",
      "Generation: 29 | Best Value: 3.73743573291\n",
      "Generation: 30 | Best Value: 3.69315785684\n",
      "Generation: 31 | Best Value: 3.51259103113\n",
      "Generation: 32 | Best Value: 3.3821895069\n",
      "Generation: 33 | Best Value: 3.07515425656\n",
      "Generation: 34 | Best Value: 2.62993033547\n",
      "Generation: 35 | Best Value: 1.79660594798\n",
      "Generation: 36 | Best Value: 1.79660594798\n",
      "Generation: 37 | Best Value: 1.79660594798\n",
      "Generation: 38 | Best Value: 1.79660594798\n",
      "Generation: 39 | Best Value: 1.79660594798\n",
      "Generation: 40 | Best Value: 1.79660594798\n",
      "Generation: 41 | Best Value: 1.7822907633\n",
      "Generation: 42 | Best Value: 1.7822907633\n",
      "Generation: 43 | Best Value: 1.7193136708\n",
      "Generation: 44 | Best Value: 1.67879907688\n",
      "Generation: 45 | Best Value: 1.67879907688\n",
      "Generation: 46 | Best Value: 1.65521271478\n",
      "Generation: 47 | Best Value: 1.38248181594\n",
      "Generation: 48 | Best Value: 1.38248181594\n",
      "Generation: 49 | Best Value: 1.38248181594\n",
      "Generation: 50 | Best Value: 1.38248181594\n",
      "Generation: 51 | Best Value: 1.31660878898\n",
      "Generation: 52 | Best Value: 1.30814818022\n",
      "Generation: 53 | Best Value: 1.24725972078\n",
      "Generation: 54 | Best Value: 1.24725972078\n",
      "Generation: 55 | Best Value: 1.20925455809\n",
      "Generation: 56 | Best Value: 1.20925455809\n",
      "Generation: 57 | Best Value: 1.14338153114\n",
      "Generation: 58 | Best Value: 1.11496968957\n",
      "Generation: 59 | Best Value: 0.971595840855\n",
      "Generation: 60 | Best Value: 0.971595840855\n",
      "Generation: 61 | Best Value: 0.912807084891\n",
      "Generation: 62 | Best Value: 0.827906474871\n",
      "Generation: 63 | Best Value: 0.827906474871\n",
      "Generation: 64 | Best Value: 0.82431646749\n",
      "Generation: 65 | Best Value: 0.710291298076\n",
      "Generation: 66 | Best Value: 0.597332028627\n",
      "Generation: 67 | Best Value: 0.597332028627\n",
      "Generation: 68 | Best Value: 0.597332028627\n",
      "Generation: 69 | Best Value: 0.597332028627\n",
      "Generation: 70 | Best Value: 0.597332028627\n",
      "Generation: 71 | Best Value: 0.58925785803\n",
      "Generation: 72 | Best Value: 0.587332327283\n",
      "Generation: 73 | Best Value: 0.587332327283\n",
      "Generation: 74 | Best Value: 0.566402712798\n",
      "Generation: 75 | Best Value: 0.566402712798\n",
      "Generation: 76 | Best Value: 0.566402712798\n",
      "Generation: 77 | Best Value: 0.412856241298\n",
      "Generation: 78 | Best Value: 0.412856241298\n",
      "Generation: 79 | Best Value: 0.370571891204\n",
      "Generation: 80 | Best Value: 0.34728502017\n",
      "Generation: 81 | Best Value: 0.341361638919\n",
      "Generation: 82 | Best Value: 0.341361638919\n",
      "Generation: 83 | Best Value: 0.325824685794\n",
      "Generation: 84 | Best Value: 0.325824685794\n",
      "Generation: 85 | Best Value: 0.325824685794\n",
      "Generation: 86 | Best Value: 0.274020471806\n",
      "Generation: 87 | Best Value: 0.274020471806\n",
      "Generation: 88 | Best Value: 0.244349794829\n",
      "Generation: 89 | Best Value: 0.222898915391\n",
      "Generation: 90 | Best Value: 0.222898915391\n",
      "Generation: 91 | Best Value: 0.222898915391\n",
      "Generation: 92 | Best Value: 0.222898915391\n",
      "Generation: 93 | Best Value: 0.222898915391\n",
      "Generation: 94 | Best Value: 0.20418621772\n",
      "Generation: 95 | Best Value: 0.191367120341\n",
      "Generation: 96 | Best Value: 0.191367120341\n",
      "Generation: 97 | Best Value: 0.191367120341\n",
      "Generation: 98 | Best Value: 0.191367120341\n",
      "Generation: 99 | Best Value: 0.191367120341\n"
     ]
    }
   ],
   "source": [
    "#This is the main genetic Algo loop in this we iterate for number of generations in each iteration we first sort our \n",
    "#parents in pop ascendingly according to their f(x) (also called fitness score). From this sorted list of vectors we\n",
    "#select 2 vectors from the good pop by slicing the list (I have taken 25 percent of the best pop) . These 2 parent vectors\n",
    "#are now allowed to cross over and children are passed into the mutation function then the two children are appended \n",
    "#to the temp vector this process is repeated till we have temp = size of pop.\n",
    "#the pop and temp(children) are combined to form comb vector which is then sorted and the best np is taken as new \n",
    "#population . At start of each new generation the least f(x) i.e. the first value in pop vector as it is sorted is taken\n",
    "#and appended to loss vector so that we can visualize it .\n",
    "loss = []\n",
    "\n",
    "# Main Genetic Algo loop\n",
    "for gx in range(G):\n",
    "    pop = sorted(pop, key=lambda z: f(z))\n",
    "    print \"Generation: {0} | Best Value: {1}\".format(gx, f(pop[0]))\n",
    "    loss.append(f(pop[0]))\n",
    "    \n",
    "    # create a temp population\n",
    "    temp = []\n",
    "    \n",
    "    while not len(temp) == NP:\n",
    "        # Select 2 parents from good section of population\n",
    "        p1, p2 = random.sample(pop[:int(NP/4.0)], 2)\n",
    "        \n",
    "        # Apply crossover\n",
    "        c1, c2 = crossover(p1, p2)\n",
    "        \n",
    "        # mutate\n",
    "        c1 = mutate(c1)\n",
    "        c2 = mutate(c2)\n",
    "        \n",
    "        temp.append(c1)\n",
    "        temp.append(c2)\n",
    "    \n",
    "    # create a combined population\n",
    "    comb = temp + pop\n",
    "    \n",
    "    # survival of the fittest\n",
    "    pop = sorted(comb, key=lambda z: f(z))[:NP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1039e8c50>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngVPQi4gETkEvIhI4Bb2ISOD+P2Afz4bLK2sUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1038ed350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We can see that we have minimized loss from 300.325 to 0.1913\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
