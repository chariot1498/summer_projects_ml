{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import keras\n",
    "#Instead of using sequential model we use model in which we have to build the network on our own we only get layers from dense.\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Converting MNIST dataset to a pandas dataframe\n",
    "ds = pd.read_csv('train.csv')\n",
    "#Only first 10000 examples are being taken due to system limitations since range of pixels is till 255 we divide by 255 to get range between 0 and 1.\n",
    "data = ds.values[:10000, 1:]/255.0\n",
    "print data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 784)               79184     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 784)               0         \n",
      "=================================================================\n",
      "Total params: 157,684\n",
      "Trainable params: 157,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 78,500\n",
      "Trainable params: 78,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 784)               79184     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 784)               0         \n",
      "=================================================================\n",
      "Total params: 79,184\n",
      "Trainable params: 79,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the layers there is one hidden layer of 100 neurons we use sigmoid activation instead of softmax function in the last layer as softmax gives us probabilities that sum to one from all the outputs of neurons but we need each output to be between 0 and 1 sum of all of them can't be 1.Thus we need sigmoid as its output is btw 0 and 1 . Tanh is not a good choice either as its range is btw -1 and 1.        \n",
    "inp = Input(shape=(784,))\n",
    "h1 = Dense(100)\n",
    "a1 = Activation('sigmoid')\n",
    "y = Dense(784,)\n",
    "ya = Activation('sigmoid')\n",
    "\n",
    "# connect layers for autoencoder we need to do this as we have model instead of sequential.\n",
    "out = ya(y(a1(h1(inp))))\n",
    "\n",
    "# Create autoencoder model we use mean square error loss function and adam optimizer categoriacal_crossEntropy \n",
    "model = Model(inputs=[inp], outputs=[out])\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Build encoder model\n",
    "encoder = Model(inputs=[inp], outputs=[a1(h1(inp))])\n",
    "encoder.summary()\n",
    "\n",
    "# Build decoder model\n",
    "dec_inp = Input(shape=(100,))\n",
    "dec_out = ya(y(dec_inp))\n",
    "decoder = Model(inputs=[dec_inp], outputs=[dec_out])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.1138 - acc: 0.0089 - val_loss: 0.0691 - val_acc: 0.0160\n",
      "Epoch 2/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0692 - acc: 0.0093 - val_loss: 0.0676 - val_acc: 0.0100\n",
      "Epoch 3/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0681 - acc: 0.0100 - val_loss: 0.0665 - val_acc: 0.0140\n",
      "Epoch 4/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0665 - acc: 0.0144 - val_loss: 0.0644 - val_acc: 0.0120\n",
      "Epoch 5/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0639 - acc: 0.0160 - val_loss: 0.0616 - val_acc: 0.0080\n",
      "Epoch 6/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0606 - acc: 0.0180 - val_loss: 0.0583 - val_acc: 0.0080\n",
      "Epoch 7/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0572 - acc: 0.0198 - val_loss: 0.0551 - val_acc: 0.0160\n",
      "Epoch 8/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0540 - acc: 0.0153 - val_loss: 0.0522 - val_acc: 0.0180\n",
      "Epoch 9/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0512 - acc: 0.0180 - val_loss: 0.0497 - val_acc: 0.0060\n",
      "Epoch 10/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0488 - acc: 0.0118 - val_loss: 0.0475 - val_acc: 0.0160\n",
      "Epoch 11/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0468 - acc: 0.0169 - val_loss: 0.0457 - val_acc: 0.0140\n",
      "Epoch 12/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0449 - acc: 0.0176 - val_loss: 0.0439 - val_acc: 0.0160\n",
      "Epoch 13/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0432 - acc: 0.0167 - val_loss: 0.0424 - val_acc: 0.0140\n",
      "Epoch 14/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0417 - acc: 0.0149 - val_loss: 0.0408 - val_acc: 0.0100\n",
      "Epoch 15/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0402 - acc: 0.0144 - val_loss: 0.0395 - val_acc: 0.0120\n",
      "Epoch 16/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0389 - acc: 0.0140 - val_loss: 0.0382 - val_acc: 0.0120\n",
      "Epoch 17/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0376 - acc: 0.0122 - val_loss: 0.0370 - val_acc: 0.0100\n",
      "Epoch 18/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0364 - acc: 0.0100 - val_loss: 0.0358 - val_acc: 0.0120\n",
      "Epoch 19/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0353 - acc: 0.0098 - val_loss: 0.0348 - val_acc: 0.0100\n",
      "Epoch 20/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0342 - acc: 0.0089 - val_loss: 0.0338 - val_acc: 0.0080\n",
      "Epoch 21/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0333 - acc: 0.0098 - val_loss: 0.0329 - val_acc: 0.0120\n",
      "Epoch 22/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0323 - acc: 0.0111 - val_loss: 0.0320 - val_acc: 0.0100\n",
      "Epoch 23/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0314 - acc: 0.0093 - val_loss: 0.0311 - val_acc: 0.0140\n",
      "Epoch 24/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0305 - acc: 0.0093 - val_loss: 0.0303 - val_acc: 0.0120\n",
      "Epoch 25/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0297 - acc: 0.0100 - val_loss: 0.0295 - val_acc: 0.0140\n",
      "Epoch 26/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0289 - acc: 0.0102 - val_loss: 0.0288 - val_acc: 0.0100\n",
      "Epoch 27/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0282 - acc: 0.0102 - val_loss: 0.0281 - val_acc: 0.0120\n",
      "Epoch 28/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0274 - acc: 0.0109 - val_loss: 0.0274 - val_acc: 0.0080\n",
      "Epoch 29/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0267 - acc: 0.0111 - val_loss: 0.0268 - val_acc: 0.0100\n",
      "Epoch 30/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0261 - acc: 0.0116 - val_loss: 0.0262 - val_acc: 0.0080\n",
      "Epoch 31/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0254 - acc: 0.0109 - val_loss: 0.0256 - val_acc: 0.0140\n",
      "Epoch 32/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0248 - acc: 0.0113 - val_loss: 0.0250 - val_acc: 0.0100\n",
      "Epoch 33/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0242 - acc: 0.0096 - val_loss: 0.0245 - val_acc: 0.0100\n",
      "Epoch 34/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0236 - acc: 0.0093 - val_loss: 0.0239 - val_acc: 0.0100\n",
      "Epoch 35/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0231 - acc: 0.0096 - val_loss: 0.0234 - val_acc: 0.0100\n",
      "Epoch 36/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0226 - acc: 0.0102 - val_loss: 0.0230 - val_acc: 0.0100\n",
      "Epoch 37/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0221 - acc: 0.0100 - val_loss: 0.0225 - val_acc: 0.0120\n",
      "Epoch 38/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0216 - acc: 0.0093 - val_loss: 0.0221 - val_acc: 0.0140\n",
      "Epoch 39/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0211 - acc: 0.0100 - val_loss: 0.0216 - val_acc: 0.0120\n",
      "Epoch 40/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0207 - acc: 0.0113 - val_loss: 0.0212 - val_acc: 0.0100\n",
      "Epoch 41/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0202 - acc: 0.0100 - val_loss: 0.0208 - val_acc: 0.0140\n",
      "Epoch 42/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0198 - acc: 0.0098 - val_loss: 0.0205 - val_acc: 0.0120\n",
      "Epoch 43/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0194 - acc: 0.0091 - val_loss: 0.0201 - val_acc: 0.0120\n",
      "Epoch 44/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0191 - acc: 0.0100 - val_loss: 0.0198 - val_acc: 0.0120\n",
      "Epoch 45/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0187 - acc: 0.0093 - val_loss: 0.0194 - val_acc: 0.0080\n",
      "Epoch 46/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0183 - acc: 0.0098 - val_loss: 0.0191 - val_acc: 0.0080\n",
      "Epoch 47/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0180 - acc: 0.0102 - val_loss: 0.0188 - val_acc: 0.0080\n",
      "Epoch 48/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0177 - acc: 0.0104 - val_loss: 0.0185 - val_acc: 0.0100\n",
      "Epoch 49/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0173 - acc: 0.0098 - val_loss: 0.0182 - val_acc: 0.0100\n",
      "Epoch 50/50\n",
      "4500/4500 [==============================] - 0s - loss: 0.0170 - acc: 0.0109 - val_loss: 0.0179 - val_acc: 0.0080\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(data[:4500], data[:4500],\n",
    "                epochs=50,\n",
    "                shuffle=True,\n",
    "                batch_size=100,\n",
    "                validation_data=(data[4500:], data[4500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "ex = encoder.predict(data[:100])\n",
    "print ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n"
     ]
    }
   ],
   "source": [
    "dx = decoder.predict(ex)\n",
    "print dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11e410c90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFJCAYAAAASfw+VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngMH/AKhDTKldkLBEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1103e7cd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFJCAYAAAASfw+VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDIQlABj+ByVUKHKiLJPoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e400f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We are now going to predict how the image looks after recreation(figure 1) varsus the original one(figure 0)\n",
    "n = np.random.choice(range(100))\n",
    "plt.figure(0)\n",
    "plt.grid('off')\n",
    "plt.imshow(data[n].reshape((28, 28)), cmap='gray')\n",
    "\n",
    "plt.figure(1)\n",
    "plt.grid('off')\n",
    "plt.imshow(dx[n].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               196250    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               25250     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 784)               196784    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 784)               0         \n",
      "=================================================================\n",
      "Total params: 443,384\n",
      "Trainable params: 443,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               196250    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 221,350\n",
      "Trainable params: 221,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               25250     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 784)               196784    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 784)               0         \n",
      "=================================================================\n",
      "Total params: 222,034\n",
      "Trainable params: 222,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the layers there is 3 hidden layer of 100 neurons we use sigmoid activation instead of softmax function in the last layer as softmax gives us probabilities that sum to one from all the outputs of neurons but we need each output to be between 0 and 1 sum of all of them can't be 1.Thus we need sigmoid as its output is btw 0 and 1 . Tanh is not a good choice either as its range is btw -1 and 1.        \n",
    "#Using multiple layers help in smooth transition making output better we can add more layers but that would come at the cost of computation cost\n",
    "inp = Input(shape=(784,))\n",
    "h1 = Dense(250)\n",
    "a1 = Activation('sigmoid')\n",
    "h2 = Dense(100)\n",
    "a2 = Activation('sigmoid')\n",
    "h3 = Dense(250)\n",
    "a3 = Activation('sigmoid')\n",
    "y = Dense(784,)\n",
    "ya = Activation('sigmoid')\n",
    "\n",
    "# connect layers for autoencoder we need to do this as we have model instead of sequential.\n",
    "out = ya(y(a3(h3(a2(h2(a1(h1(inp))))))))\n",
    "\n",
    "# Create autoencoder model we use mean square error loss function and adam optimizer categoriacal_crossEntropy \n",
    "model = Model(inputs=[inp], outputs=[out])\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Build encoder model\n",
    "encoder = Model(inputs=[inp], outputs=[a2(h2(a1(h1(inp))))])\n",
    "encoder.summary()\n",
    "\n",
    "# Build decoder model\n",
    "dec_inp = Input(shape=(100,))\n",
    "dec_out = ya(y(a3(h3(dec_inp))))\n",
    "decoder = Model(inputs=[dec_inp], outputs=[dec_out])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 2500 samples\n",
      "Epoch 1/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0824 - acc: 0.0089 - val_loss: 0.0680 - val_acc: 0.0092\n",
      "Epoch 2/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0673 - acc: 0.0104 - val_loss: 0.0664 - val_acc: 0.0072\n",
      "Epoch 3/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0639 - acc: 0.0099 - val_loss: 0.0623 - val_acc: 0.0164\n",
      "Epoch 4/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0611 - acc: 0.0141 - val_loss: 0.0597 - val_acc: 0.0056\n",
      "Epoch 5/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0560 - acc: 0.0116 - val_loss: 0.0531 - val_acc: 0.0096\n",
      "Epoch 6/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0520 - acc: 0.0121 - val_loss: 0.0509 - val_acc: 0.0060\n",
      "Epoch 7/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0490 - acc: 0.0101 - val_loss: 0.0467 - val_acc: 0.0096\n",
      "Epoch 8/50\n",
      "7500/7500 [==============================] - 2s - loss: 0.0450 - acc: 0.0071 - val_loss: 0.0440 - val_acc: 0.0052\n",
      "Epoch 9/50\n",
      "7500/7500 [==============================] - 2s - loss: 0.0427 - acc: 0.0068 - val_loss: 0.0416 - val_acc: 0.0040\n",
      "Epoch 10/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0403 - acc: 0.0076 - val_loss: 0.0397 - val_acc: 0.0096\n",
      "Epoch 11/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0387 - acc: 0.0084 - val_loss: 0.0383 - val_acc: 0.0072\n",
      "Epoch 12/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0370 - acc: 0.0084 - val_loss: 0.0362 - val_acc: 0.0100\n",
      "Epoch 13/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0351 - acc: 0.0104 - val_loss: 0.0346 - val_acc: 0.0076\n",
      "Epoch 14/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0337 - acc: 0.0124 - val_loss: 0.0333 - val_acc: 0.0096\n",
      "Epoch 15/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0324 - acc: 0.0148 - val_loss: 0.0321 - val_acc: 0.01400.01\n",
      "Epoch 16/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0313 - acc: 0.0137 - val_loss: 0.0311 - val_acc: 0.0136\n",
      "Epoch 17/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0302 - acc: 0.0129 - val_loss: 0.0300 - val_acc: 0.0076\n",
      "Epoch 18/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0292 - acc: 0.0111 - val_loss: 0.0290 - val_acc: 0.0124\n",
      "Epoch 19/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0283 - acc: 0.0124 - val_loss: 0.0283 - val_acc: 0.0096\n",
      "Epoch 20/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0275 - acc: 0.0132 - val_loss: 0.0276 - val_acc: 0.0124\n",
      "Epoch 21/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0268 - acc: 0.0117 - val_loss: 0.0270 - val_acc: 0.0116\n",
      "Epoch 22/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0261 - acc: 0.0112 - val_loss: 0.0264 - val_acc: 0.0128\n",
      "Epoch 23/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0254 - acc: 0.0113 - val_loss: 0.0256 - val_acc: 0.0096\n",
      "Epoch 24/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0246 - acc: 0.0127 - val_loss: 0.0247 - val_acc: 0.0104\n",
      "Epoch 25/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0237 - acc: 0.0120 - val_loss: 0.0240 - val_acc: 0.0112\n",
      "Epoch 26/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0230 - acc: 0.0136 - val_loss: 0.0233 - val_acc: 0.0112\n",
      "Epoch 27/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0224 - acc: 0.0115 - val_loss: 0.0228 - val_acc: 0.0140\n",
      "Epoch 28/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0219 - acc: 0.0115 - val_loss: 0.0223 - val_acc: 0.0108\n",
      "Epoch 29/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0214 - acc: 0.0116 - val_loss: 0.0218 - val_acc: 0.0108\n",
      "Epoch 30/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0209 - acc: 0.0107 - val_loss: 0.0214 - val_acc: 0.0080\n",
      "Epoch 31/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0204 - acc: 0.0105 - val_loss: 0.0209 - val_acc: 0.0140\n",
      "Epoch 32/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0200 - acc: 0.0108 - val_loss: 0.0204 - val_acc: 0.0124\n",
      "Epoch 33/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0196 - acc: 0.0099 - val_loss: 0.0200 - val_acc: 0.0112\n",
      "Epoch 34/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0192 - acc: 0.0105 - val_loss: 0.0196 - val_acc: 0.0084\n",
      "Epoch 35/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0188 - acc: 0.0108 - val_loss: 0.0193 - val_acc: 0.0148\n",
      "Epoch 36/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0184 - acc: 0.0095 - val_loss: 0.0189 - val_acc: 0.0096\n",
      "Epoch 37/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0180 - acc: 0.0107 - val_loss: 0.0186 - val_acc: 0.0076\n",
      "Epoch 38/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0177 - acc: 0.0109 - val_loss: 0.0183 - val_acc: 0.0084\n",
      "Epoch 39/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0174 - acc: 0.0119 - val_loss: 0.0180 - val_acc: 0.0088\n",
      "Epoch 40/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0170 - acc: 0.0104 - val_loss: 0.0177 - val_acc: 0.0072\n",
      "Epoch 41/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0167 - acc: 0.0116 - val_loss: 0.0174 - val_acc: 0.0080\n",
      "Epoch 42/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0164 - acc: 0.0107 - val_loss: 0.0171 - val_acc: 0.0076\n",
      "Epoch 43/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0161 - acc: 0.0109 - val_loss: 0.0169 - val_acc: 0.0064\n",
      "Epoch 44/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0159 - acc: 0.0105 - val_loss: 0.0166 - val_acc: 0.0080\n",
      "Epoch 45/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0156 - acc: 0.0096 - val_loss: 0.0164 - val_acc: 0.0072\n",
      "Epoch 46/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0153 - acc: 0.0108 - val_loss: 0.0161 - val_acc: 0.0068\n",
      "Epoch 47/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0150 - acc: 0.0112 - val_loss: 0.0158 - val_acc: 0.0096\n",
      "Epoch 48/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0147 - acc: 0.0112 - val_loss: 0.0156 - val_acc: 0.0084\n",
      "Epoch 49/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0145 - acc: 0.0103 - val_loss: 0.0154 - val_acc: 0.0104\n",
      "Epoch 50/50\n",
      "7500/7500 [==============================] - 1s - loss: 0.0142 - acc: 0.0103 - val_loss: 0.0152 - val_acc: 0.0096\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(data[:7500], data[:7500],\n",
    "                epochs=50,\n",
    "                shuffle=True,\n",
    "                batch_size=100,\n",
    "                validation_data=(data[7500:], data[7500:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "ex = encoder.predict(data[:100])\n",
    "print ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n"
     ]
    }
   ],
   "source": [
    "dx = decoder.predict(ex)\n",
    "print dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x120c8b690>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAAD3CAYAAADfRfLgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e1b0e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAAD3CAYAAADfRfLgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyVFyIDlKDiRHyYHk/hvPkRVfrZOn8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124c20950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = np.random.choice(range(100))\n",
    "plt.figure(0)\n",
    "plt.grid('off')\n",
    "plt.imshow(data[n].reshape((28, 28)), cmap='gray')\n",
    "\n",
    "plt.figure(1)\n",
    "plt.grid('off')\n",
    "plt.imshow(dx[n].reshape((28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
