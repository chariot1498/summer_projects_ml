{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#I am going to use keras for creating convolution neural network\n",
    "import keras\n",
    "#I am using sequential model . model from keras.models can also be used and create layers as functions \n",
    "from keras.layers import Convolution2D, Flatten, Dense, MaxPool2D, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "# Data loading you need to download mnist from kaggle and keep it in the same directory as this notebook to run this cell\n",
    "\n",
    "ds = pd.read_csv('train.csv')\n",
    "ds.shape\n",
    "data = ds.values\n",
    "print data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[445][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dividing data in da digits less than 5 others in da2\n",
    "da = []\n",
    "da2 = []\n",
    "for i in range(data.shape[0]):\n",
    "    if data[i][0] < 5 :\n",
    "        da.append(data[i])\n",
    "    else :\n",
    "        da2.append(data[i])\n",
    "da = np.array(da)        \n",
    "da2 = np.array(da2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21416, 785)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21416, 784) (21416, 5)\n"
     ]
    }
   ],
   "source": [
    "#converting range of each pixel from 0 to 1.\n",
    "X = da[:, 1:]/255.0\n",
    "#Converting each y from just the class value(42000,1) to a vector representing 1 for the class (42000,10)\n",
    "y = np_utils.to_categorical(da[:, 0])\n",
    "\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20584, 784) (20584, 5)\n"
     ]
    }
   ],
   "source": [
    "#converting range of each pixel from 0 to 1.\n",
    "X2 = da2[:, 1:]/255.0\n",
    "#Converting each y from just the class value(42000,1) to a vector representing 1 for the class (42000,10)\n",
    "y2 = np_utils.to_categorical(da2[:, 0]-5,num_classes = 5)\n",
    "\n",
    "print X2.shape, y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17132, 28, 28, 1) (4284, 28, 28, 1)\n",
      "(17132, 5) (4284, 5)\n"
     ]
    }
   ],
   "source": [
    "#Creating training and testing data\n",
    "split = int(0.8 * X.shape[0])\n",
    "\n",
    "X_train = X[:split].reshape((-1, 28, 28, 1))\n",
    "X_test = X[split:].reshape((-1, 28, 28, 1))\n",
    "\n",
    "y_train = y[:split]\n",
    "y_test = y[split:]\n",
    "\n",
    "print X_train.shape, X_test.shape\n",
    "print y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16467, 28, 28, 1) (4117, 28, 28, 1)\n",
      "(16467, 5) (4117, 5)\n"
     ]
    }
   ],
   "source": [
    "#Creating training and testing data\n",
    "split = int(0.8 * X2.shape[0])\n",
    "\n",
    "X2_train = X2[:split].reshape((-1, 28, 28, 1))\n",
    "X2_test = X2[split:].reshape((-1, 28, 28, 1))\n",
    "\n",
    "y2_train = y2[:split]\n",
    "y2_test = y2[split:]\n",
    "\n",
    "print X2_train.shape, X2_test.shape\n",
    "print y2_train.shape, y2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118b20fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_img = 21\n",
    "#Just printing a random image in the mnist dataset\n",
    "plt.imshow(X_train[n_img].reshape((28, 28)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 24, 24, 16)        4624      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 10, 10, 8)         1160      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 10, 10, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 5, 5, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 7,109\n",
      "Trainable params: 7,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model sequential \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#adding the first convolution layer with 32 kernels and kernel size 3*3.Since it is rgb we have 1 feature map in the\n",
    "#input image. this will output an image with 26,26,32 32 feature maps for each kernel even if the image was rgb\n",
    "#we would have still got 32 feature maps as each kernel would have calculated a feature map for each of the three and \n",
    "#taken it's average. relu seems to be a nice activation in this case though sigmoid would also work.Parameter involved\n",
    "#would be 32*9(weights inside each kernel) + 32(bias of each kernel) .\n",
    "\n",
    "model.add(Convolution2D(32, (3, 3), input_shape=(28, 28, 1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Second layer with 16 kernels and each kernel 3*3 it gives us 24,24,16 i.e. 16 feature maps. The number of parameters \n",
    "#are 16*9*32(wieghts inside kernel) + 16(bias of each kernel)\n",
    "\n",
    "model.add(Convolution2D(16, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#We add a layer of maxpool in this the kernel(size = 2,2) fits over 2*2 block of image and takes the max value from the\n",
    "#4 cells thus the image is of size = 24/2,24/2,16 = 12,12,16\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "#Third layer of convolution with 8 kernels of size 3*3 this results in an image of size 10,10,8 with 8 feature maps \n",
    "#and 8*16*9(weights inside kernel)+8(bias) = 1160 parameters\n",
    "\n",
    "model.add(Convolution2D(8, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Second maxpool layer again of size 2*2 thus resulting image is of size = 10/2,10/2,8 = 5,5,8\n",
    "\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "#flatten the image to 5*5*8 = 200 (200,1) so that final pred could be done as final layer is dense it needs a vector \n",
    "#while convolutions needed a matrix\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#To prevent overfitting drop out is added.\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "#final layer for prediction it is using softmax to calculate probability for the 10 classes. \n",
    "\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#We use categorical_crossentropy as loss function and adam as the optimizer . \n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17132 samples, validate on 4284 samples\n",
      "Epoch 1/4\n",
      "17132/17132 [==============================] - 25s - loss: 0.5177 - acc: 0.8094 - val_loss: 0.0839 - val_acc: 0.9725\n",
      "Epoch 2/4\n",
      "17132/17132 [==============================] - 25s - loss: 0.1213 - acc: 0.9632 - val_loss: 0.0524 - val_acc: 0.9853\n",
      "Epoch 3/4\n",
      "17132/17132 [==============================] - 25s - loss: 0.0850 - acc: 0.9741 - val_loss: 0.0399 - val_acc: 0.9883\n",
      "Epoch 4/4\n",
      "17132/17132 [==============================] - 25s - loss: 0.0702 - acc: 0.9769 - val_loss: 0.0358 - val_acc: 0.9904\n"
     ]
    }
   ],
   "source": [
    "#training takes over 8 minutes on my cpu based mac having gpu would make the whole time faster . train it for da\n",
    "hist = model.fit(X_train, y_train,\n",
    "                epochs=4,\n",
    "                shuffle=True,\n",
    "                batch_size=100,\n",
    "                validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x1192ec9d0>,\n",
       " <keras.layers.core.Activation at 0x1192ec710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1180121d0>,\n",
       " <keras.layers.core.Activation at 0x1192c2650>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x118ba1b90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1180120d0>,\n",
       " <keras.layers.core.Activation at 0x118012d10>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x11801eb10>,\n",
       " <keras.layers.core.Flatten at 0x1192cf990>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 24, 24, 16)        4624      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 10, 10, 8)         1160      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 10, 10, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 5, 5, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 1005      \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 7,109\n",
      "Trainable params: 1,005\n",
      "Non-trainable params: 6,104\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#took the convolutional layers and turned their training of and added new dense(output) layer for da2\n",
    "trans_model = Sequential(model.layers[:-3])\n",
    "for ix in trans_model.layers :\n",
    "    ix.trainable = False\n",
    "trans_model.add(Dense(5))\n",
    "trans_model.add(Activation('softmax'))\n",
    "trans_model.summary()\n",
    "\n",
    "trans_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16467 samples, validate on 4117 samples\n",
      "Epoch 1/4\n",
      "16467/16467 [==============================] - 9s - loss: 0.5949 - acc: 0.8026 - val_loss: 0.2266 - val_acc: 0.9439\n",
      "Epoch 2/4\n",
      "16467/16467 [==============================] - 9s - loss: 0.1802 - acc: 0.9523 - val_loss: 0.1432 - val_acc: 0.9658\n",
      "Epoch 3/4\n",
      "16467/16467 [==============================] - 11s - loss: 0.1295 - acc: 0.9648 - val_loss: 0.1132 - val_acc: 0.9728\n",
      "Epoch 4/4\n",
      "16467/16467 [==============================] - 9s - loss: 0.1065 - acc: 0.9701 - val_loss: 0.0940 - val_acc: 0.9755\n"
     ]
    }
   ],
   "source": [
    "#training takes over 8 minutes on my cpu based mac having gpu would make the whole time faster .run it for da2\n",
    "hist = trans_model.fit(X2_train, y2_train,\n",
    "                epochs=4,\n",
    "                shuffle=True,\n",
    "                batch_size=100,\n",
    "                validation_data=(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
